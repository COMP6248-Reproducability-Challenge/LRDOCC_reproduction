{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COMP6248_K9DE_BaselineR1850.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VulfGqxro3Y"
      },
      "source": [
        "#COMP6248 K9DE\n",
        "Reproduction of baseline Resnet 18/50 results.  First, packages are loaded and functions defined.  Secondly (if you scroll to the very bottom!), runs are performed - a minimal example of how to perform one run is given, plus the code to reproduce the baseline metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R1OXj1Frzr-"
      },
      "source": [
        "#Load packages and team functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NckJrtMkbB3e",
        "outputId": "766ba60d-4f0f-4f53-ae7c-30014be98a28"
      },
      "source": [
        "!nvidia-smi\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "try: \n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install torchbearer\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torch import optim\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet50\n",
        "from urllib.request import urlopen\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 13 07:48:45 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Collecting torchbearer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e9/4049a47dd2e5b6346a2c5d215b0c67dce814afbab1cd54ce024533c4834e/torchbearer-0.5.3-py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchbearer) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->torchbearer) (3.7.4.3)\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ6r1IyX8IUZ"
      },
      "source": [
        "def K9_dataloader(dataset=\"fMNIST\",batch_size=16,shuffle_train_set=False):\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # COMP6248 Team K9 Density Estimators dataloader\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Input: dataset e{\"fMNIST\",\"CIFAR10\",\"CIFAR100\"}, other arguments are self explanatory\n",
        "  # Outputs: train and test loaders\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Author: Ian\n",
        "  # Reviewed by: Nic\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  import torchvision\n",
        "  import torchvision.transforms as transforms\n",
        "  import torch \n",
        "  import torch.nn.functional as F\n",
        "  from torch import nn\n",
        "  import torchbearer\n",
        "  from torchbearer import Trial\n",
        "  from torch import optim\n",
        "  from torchvision.models import resnet18\n",
        "  from torchvision.models import resnet50\n",
        "  from urllib.request import urlopen\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  #Setup the preprocess function\n",
        "  if dataset == \"fMNIST\":\n",
        "    preprocess_input = transforms.Compose([\n",
        "                transforms.Resize(256),    #resize 256, then centercrop 224; as per augment.py line 93 and line 97\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.Grayscale(3),   #\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])])   #this is in one of their python scripts, and is standard for ImageNet\n",
        "  else:\n",
        "    preprocess_input = transforms.Compose([    #no Grayscale for CIFAR10/CIFAR100\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  #Download the data\n",
        "  if dataset ==\"fMNIST\":\n",
        "    train_set = torchvision.datasets.FashionMNIST(root='./data/FashionMNIST',train=True,download=True,transform=preprocess_input)\n",
        "    test_set = torchvision.datasets.FashionMNIST(root='./data/FashionMNIST',train=False,download=True,transform=preprocess_input)\n",
        "  if dataset ==\"CIFAR10\":\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data/CIFAR10',train=True,download=True,transform=preprocess_input)\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data/CIFAR10',train=False,download=True,transform=preprocess_input)\n",
        "  if dataset ==\"CIFAR100\":\n",
        "    train_set = torchvision.datasets.CIFAR100(root='./data/CIFAR100',train=True,download=True,transform=preprocess_input)\n",
        "    test_set = torchvision.datasets.CIFAR100(root='./data/CIFAR100',train=False,download=True,transform=preprocess_input)\n",
        "\n",
        "  #Setup the loaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=shuffle_train_set)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        "\n",
        "  print()\n",
        "  print(f\"train_loader batches: {len(train_loader)}, test_loader batches: {len(test_loader)}, batch_size: {batch_size}\")\n",
        "  print(f\"train_shuffle set to {shuffle_train_set}\")\n",
        "  print(f\"train_set length: {len(train_set)}, test_set length: {len(test_set)}\")\n",
        "  print(f\"train_loader and test_loader ready for {dataset}.\")\n",
        "  print()\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcxrAsHvBj8y"
      },
      "source": [
        "def K9_resnet1850_ftex(train_loader, test_loader, mdltype=\"resnet18\", pretrained=False):\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # COMP6248 Team K9 Density Estimators feature extractor for resnet 18/50\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Input: mldtype e{\"resnet18\",\"resnet50\"}, other arguments self explanatory\n",
        "  # Outputs: 1D array of class AUCs\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Author: Ian\n",
        "  # Reviewed by: Nic\n",
        "  ########################################################\n",
        "  !nvidia-smi\n",
        "  try:\n",
        "      import torch\n",
        "  except:\n",
        "      from os.path import exists\n",
        "      from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "      platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "      cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "      accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "      !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "  try: \n",
        "      import torchbearer\n",
        "  except:\n",
        "      !pip install torchbearer\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  ################################################################################\n",
        "  ############### EXTRACT FEATURES FROM RESNET // based upon https://github.com/ecs-vlc/COMP6248/blob/master/docs/labs/lab6/genfeats.py\n",
        "  ################################################################################\n",
        "  import torchvision\n",
        "  import torchvision.transforms as transforms\n",
        "  import torch \n",
        "  import torch.nn.functional as F\n",
        "  from torch import nn\n",
        "  import torchbearer\n",
        "  from torchbearer import Trial\n",
        "  from torch import optim\n",
        "  from torchvision.models import resnet18\n",
        "  from torchvision.models import resnet50\n",
        "  from urllib.request import urlopen\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  torch.cuda.empty_cache(); import gc; gc.collect()    #to try to avoid the CUDA out of memory issues...\n",
        "\n",
        "  if mdltype == \"resnet18\":\n",
        "    model = resnet18(pretrained=pretrained)\n",
        "  if mdltype == \"resnet50\":\n",
        "    model = resnet50(pretrained=pretrained)\n",
        "\n",
        "  feature_extractor_model = nn.Sequential(*list(model.children())[:-2], nn.AdaptiveAvgPool2d((1,1)))\n",
        "  feature_extractor_model.eval()\n",
        "  feature_extractor_model = feature_extractor_model.to(device)\n",
        "\n",
        "  import numpy as np\n",
        "  ##############\n",
        "  #train set to vects\n",
        "  ##############\n",
        "  temp_ft, temp_lb = [], []\n",
        "  i = 0\n",
        "  for dt, lb in train_loader:\n",
        "    if i % 500 == 0: print(f\"Extracting batch {i} from train_loader.\")\n",
        "    i+=1\n",
        "    tempfts = feature_extractor_model(dt.to(device))\n",
        "    for j in range(tempfts.shape[0]):\n",
        "      temp_ft.append(tempfts[j].reshape(-1).cpu().detach().numpy())\n",
        "      temp_lb.append(lb[j].detach().numpy())\n",
        "\n",
        "  train_ft = np.array(temp_ft)\n",
        "  train_lb = np.array(temp_lb)\n",
        "  ##############\n",
        "  #test set to vects\n",
        "  ##############\n",
        "  temp_ft, temp_lb = [], []\n",
        "  i = 0\n",
        "  for dt, lb in test_loader:\n",
        "    if i % 500 == 0: print(f\"Extracting batch {i} from test_loader.\")\n",
        "    i+=1\n",
        "    tempfts = feature_extractor_model(dt.to(device))\n",
        "    for j in range(tempfts.shape[0]):\n",
        "      temp_ft.append(tempfts[j].reshape(-1).cpu().detach().numpy())\n",
        "      temp_lb.append(lb[j].detach().numpy())\n",
        "\n",
        "  test_ft = np.array(temp_ft)\n",
        "  test_lb = np.array(temp_lb)\n",
        "\n",
        "  print()\n",
        "  print(f\"train_ft shape: {train_ft.shape}\")\n",
        "  print(f\"train_lb shape: {train_lb.shape}\")\n",
        "  print(f\"test_ft shape:  {test_ft.shape}\")\n",
        "  print(f\"test_ft shape:  {test_lb.shape}\")\n",
        "  print(f\"Feature extraction from {mdltype} (pretrained={pretrained}), is complete.\")\n",
        "  print()\n",
        "\n",
        "  return train_ft, train_lb, test_ft, test_lb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbjism9vKDCl"
      },
      "source": [
        "def K9_stratified_class_sample(data_ft, data_lb, samp_per_cls=500, random_seed = False):\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # COMP6248 Team K9 Density Estimators stratified sampler\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Author: Ian\n",
        "  # Reviewed by: Nic\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  import numpy as np\n",
        "  # clss = np.unique(data_lb)\n",
        "  # ncls = len(clss)\n",
        "  # samp_per_cls = 5000\n",
        "  print(f\"Choosing {samp_per_cls} samples per class.  data_lb has {len(np.unique(data_lb))} classes.\")\n",
        "\n",
        "  idxx = np.array([],dtype=\"int\")\n",
        "  for cls in np.unique(data_lb):\n",
        "    idxx = np.append(idxx,\n",
        "                    np.random.choice(np.where(data_lb == cls)[0],size=samp_per_cls))\n",
        "\n",
        "  data_ft_sampled, data_lb_sampled = data_ft[idxx,:], data_lb[idxx]\n",
        "\n",
        "  #check\n",
        "  print(\"Summary of data_lb_sampled class sample size:\")\n",
        "  i = 1\n",
        "  for cls in np.unique(data_lb):\n",
        "    if i % 10 != 0:\n",
        "      print(f\"Cls {cls}: {np.sum(data_lb_sampled == cls)}\",end= \" | \")\n",
        "    else:\n",
        "      print(f\"Cls {cls}: {np.sum(data_lb_sampled == cls)}\")\n",
        "    i += 1\n",
        "  \n",
        "  return data_ft_sampled, data_lb_sampled"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRAebgfXzzg0"
      },
      "source": [
        "def K9_OCSVM_v4(X_train, y_train, X_test, y_test, kernel_type='rbf', balance_testinoutlier=False,\n",
        "                gammasklearndefault=False,specificgamma=None,papergammanumerator=10.0):\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # COMP6248 Team K9 Density Estimators reproduction of OC-SVM logic based on paper's train.py specification\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Input: features and labels for train and test sets;\n",
        "  #        kernel_type e{'rbf','linear'}\n",
        "  #        balance_test_inoutlier: if True, inlier and outlier test sets are balanced by stratified sampling of outliers\n",
        "  #        gammasklearndefault: if True, for RBF OC-SVM, sklearn default gamma is used;\n",
        "  #        specificgamma: if None, the formula from the paper is used.  Else, the specific gamma to be used may be specified.\n",
        "  #        papergammanumerator: 10.0 by default, the value of the numerator of the gamma function in the paper.  Setting to 1.0 would\n",
        "  #                               yield the same value as sklearn default.  8.0 has been shown to yield results of interest for some runs.\n",
        "  # Outputs: 1D array of class AUCs\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Author: Ian\n",
        "  # Reviewed by: Marios\n",
        "  ########################################################\n",
        "  #Paper code: excerpts from train.py\n",
        "  ########################################################\n",
        "  # RBF kernel OC-SVM.\n",
        "  #         feats_tr = tf.nn.l2_normalize(feats_tr, axis=1)\n",
        "  #         feats = tf.nn.l2_normalize(feats, axis=1)\n",
        "  #         # 10 times larger value of gamma.\n",
        "  #         gamma = 10. / (tf.math.reduce_variance(feats_tr) * feats_tr.shape[1])\n",
        "  #         clf = OneClassSVM(kernel='rbf', gamma=gamma).fit(feats_tr)\n",
        "  #         scores = -clf.score_samples(feats)\n",
        "  #         self.eval_metrics[key] = util_metric.roc(pr=scores, gt=labels)\n",
        "  # Linear OC-SVM.\n",
        "  #       elif 'locsvm' in key and key.startswith(('embed', 'pool')):\n",
        "  #         # Linear kernel OC-SVM.\n",
        "  #         clf = OneClassSVM(kernel='linear').fit(feats_tr)\n",
        "  #         scores = -clf.score_samples(feats)\n",
        "  #         self.eval_metrics[key] = util_metric.roc(pr=scores, gt=labels)\n",
        "  ########################################################\n",
        "  #Marios review of Ian code 06/05/21:\n",
        "  ########################################################\n",
        "  ## 1) try tf.nn.l2_normalise vs sklearn normalise.  Ian: tested on f-MNIST+checked the documentation; got exactly the same results, pretty sure they are the same.\n",
        "  ## 2) double check the -1 or 0 business  - does it make a difference? Ian: tested, makes no difference; documentation states it works off the class with 'the greater label'.\n",
        "\n",
        "  import sklearn\n",
        "  from sklearn.pipeline import make_pipeline\n",
        "  from sklearn.preprocessing import normalize\n",
        "  from sklearn.svm import OneClassSVM\n",
        "  from sklearn import metrics\n",
        "  from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "  print(\"Starting K9 OC-SVM, kernel_type = \"+kernel_type)\n",
        "  print(f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}\")\n",
        "  print(f\"X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\")\n",
        "  print()\n",
        "  train_classes, per_class_auc, one_point_per_class_auc = np.unique(y_train), [], []\n",
        "  for one_class in train_classes:\n",
        "    #one_class = train_classes[1]\n",
        "    #Normalise train set; set gamma (both as per the paper)\n",
        "    OC_X_train = X_train[y_train==one_class,:]\n",
        "    OC_X_train_normalised = normalize(OC_X_train,norm='l2',axis=1)   #normalise by L2 norm on a **row** basis\n",
        "    \n",
        "    #Fit the OC-SVM\n",
        "    print(f\"np.var(OC_X_train_normalised):{np.var(OC_X_train_normalised)}\")\n",
        "    if kernel_type == 'rbf':\n",
        "      if gammasklearndefault:\n",
        "        clf = OneClassSVM(kernel='rbf')\n",
        "        print(f\"gamma: as per sklearn default.\")\n",
        "      else:\n",
        "        if specificgamma == None:\n",
        "          gamma = papergammanumerator/(np.var(OC_X_train_normalised) * OC_X_train_normalised.shape[1])\n",
        "          #gamma = papergammanumerator/(np.var(OC_X_train) * OC_X_train_normalised.shape[1])\n",
        "          print(f\"gamma: as per paper: {gamma}\")\n",
        "        else:\n",
        "          gamma = specificgamma\n",
        "          print(f\"gamma: specified by user: {gamma}\")\n",
        "        clf = OneClassSVM(kernel='rbf', gamma=gamma)\n",
        "    else:\n",
        "      clf = OneClassSVM(kernel='linear')\n",
        "    clf = clf.fit(OC_X_train_normalised)\n",
        "\n",
        "    #Balance data, if specified\n",
        "    if balance_testinoutlier == True:\n",
        "      idxx = np.where(y_test == one_class)[0]\n",
        "      idxx = np.append(idxx,\n",
        "                np.random.choice(np.where(y_test != one_class)[0],\n",
        "                        size=len(idxx),\n",
        "                        replace=False))\n",
        "      X_test_cls = X_test[idxx]\n",
        "      y_test_cls = y_test[idxx]\n",
        "    else:\n",
        "      X_test_cls = X_test\n",
        "      y_test_cls = y_test\n",
        "\n",
        "    #Use fitted model to make predictions on test\n",
        "    X_test_normalised = normalize(X_test_cls,norm='l2',axis=1)   #normalise by L2 norm on a **row** basis\n",
        "    y_test_pred = clf.predict(X_test_normalised)\n",
        "    y_test_pred_scores = clf.score_samples(X_test_normalised)\n",
        "    y_test_pred_AUC = roc_auc_score(1.*(y_test_cls==one_class),y_test_pred_scores)\n",
        "\n",
        "    #save AUC\n",
        "    per_class_auc = np.append(per_class_auc, y_test_pred_AUC)\n",
        "\n",
        "    #Print out results\n",
        "    y_test_tp, y_test_tn = np.sum(y_test_pred[y_test_cls==one_class] == 1), np.sum(y_test_pred[y_test_cls!=one_class] == -1)\n",
        "    y_test_fp, y_test_fn = np.sum(y_test_pred[y_test_cls!=one_class] == 1), np.sum(y_test_pred[y_test_cls==one_class] == -1)\n",
        "    y_test_n_inlier, y_test_n_outlier = np.sum(y_test_cls==one_class), np.sum(y_test_cls!=one_class)\n",
        "    y_test_fpr, y_test_tpr = y_test_fp/(y_test_fp+y_test_tn), y_test_tp/(y_test_tp+y_test_fn)\n",
        "    y_test_one_point_auc = 0.5*(1-y_test_fpr+y_test_tpr)\n",
        "    #save one_point_auc\n",
        "    one_point_per_class_auc = np.append(one_point_per_class_auc, y_test_one_point_auc)\n",
        "    print(f\"Class: {one_class}\")\n",
        "    print(f\"AUC score: {y_test_pred_AUC: .4f}, Accuracy: {(y_test_tp+y_test_tn)/len(y_test_cls): .4f}\")\n",
        "    print(f\"OC_X_train shape: {OC_X_train.shape}, X_test_cls shape: {X_test_cls.shape}.\")\n",
        "    print(f\"Results after model application to *test* set: n_inlier: {y_test_n_inlier} ; n_outlier: {y_test_n_outlier}\")\n",
        "    print(f\"TP: {y_test_tp}, TN: {y_test_tn}, FP: {y_test_fp}, FN: {y_test_fn}\")\n",
        "    print(f\"TPR: {y_test_tpr}, FPR: {y_test_fpr}, one-point AUC:{y_test_one_point_auc}\")\n",
        "    print()\n",
        "  print(f\"Unweighted mean AUC: {np.mean(per_class_auc): .4f}\")\n",
        "  print(f\"Unweighted one-point mean AUC: {np.mean(one_point_per_class_auc): .4f}\")\n",
        "  return per_class_auc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjf38bxoRcJe"
      },
      "source": [
        "def K9_pd_runsummary():\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # COMP6248 Team K9 Density Estimators summary of run configuration\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  # Author: Ian\n",
        "  # Reviewed by: Nic\n",
        "  #*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*\n",
        "  import pandas as pd\n",
        "  run_params  = pd.DataFrame([dataset, batch_size, mdltype, pretrained, samp_per_cls, random_seed, kernel_type,\n",
        "                        X_train.shape[0], X_train.shape[1], y_train.shape[0], X_test.shape[0], X_test.shape[1], y_test.shape[0],np.mean(res)])\n",
        "  run_results = pd.DataFrame(res)\n",
        "  run_summary = pd.concat([run_params,run_results],ignore_index=True)\n",
        "  return run_summary"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQjbenirr81t"
      },
      "source": [
        "#Run One Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O__64U6Frlpv"
      },
      "source": [
        "Minimal example of loading and running one run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp2PhV5HASYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7340e0c4-8cc4-471c-ced4-795fe7d25608"
      },
      "source": [
        "#Minimal example of loading and running one run\n",
        "train_loader, test_loader = K9_dataloader(dataset=\"CIFAR10\", batch_size=128)\n",
        "train_ft, train_lb, test_ft, test_lb = K9_resnet1850_ftex(train_loader, test_loader, mdltype=\"resnet18\", pretrained=False)\n",
        "X_train, y_train = K9_stratified_class_sample(train_ft, train_lb, samp_per_cls=5000)\n",
        "X_test, y_test   = test_ft, test_lb\n",
        "res = K9_OCSVM_v4(X_train, y_train, X_test, y_test, kernel_type='rbf', balance_testinoutlier=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "train_loader batches: 391, test_loader batches: 79, batch_size: 128\n",
            "train_shuffle set to False\n",
            "train_set length: 50000, test_set length: 10000\n",
            "train_loader and test_loader ready for CIFAR10.\n",
            "\n",
            "Thu May 13 08:04:06 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    34W / 250W |   4093MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Extracting batch 0 from train_loader.\n",
            "Extracting batch 0 from test_loader.\n",
            "\n",
            "train_ft shape: (50000, 512)\n",
            "train_lb shape: (50000,)\n",
            "test_ft shape:  (10000, 512)\n",
            "test_ft shape:  (10000,)\n",
            "Feature extraction from resnet18 (pretrained=False), is complete.\n",
            "\n",
            "Choosing 5000 samples per class.  data_lb has 10 classes.\n",
            "Summary of data_lb_sampled class sample size:\n",
            "Cls 0: 5000 | Cls 1: 5000 | Cls 2: 5000 | Cls 3: 5000 | Cls 4: 5000 | Cls 5: 5000 | Cls 6: 5000 | Cls 7: 5000 | Cls 8: 5000 | Cls 9: 5000\n",
            "Starting K9 OC-SVM, kernel_type = rbf\n",
            "X_train.shape: (50000, 512), y_train.shape: (50000,)\n",
            "X_test.shape: (10000, 512), y_test.shape: (10000,)\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009168298565782607\n",
            "gamma: as per paper: 21.30302570303873\n",
            "Class: 0\n",
            "AUC score:  0.5401, Accuracy:  0.5215\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 522, TN: 521, FP: 479, FN: 478\n",
            "TPR: 0.522, FPR: 0.479, one-point AUC:0.5215000000000001\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009072532411664724\n",
            "gamma: as per paper: 21.52789222873242\n",
            "Class: 1\n",
            "AUC score:  0.6690, Accuracy:  0.6125\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 517, TN: 708, FP: 292, FN: 483\n",
            "TPR: 0.517, FPR: 0.292, one-point AUC:0.6125\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009073513792827725\n",
            "gamma: as per paper: 21.5255637958458\n",
            "Class: 2\n",
            "AUC score:  0.4302, Accuracy:  0.4405\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 486, TN: 395, FP: 605, FN: 514\n",
            "TPR: 0.486, FPR: 0.605, one-point AUC:0.4405\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009055458358488977\n",
            "gamma: as per paper: 21.56848303729492\n",
            "Class: 3\n",
            "AUC score:  0.5528, Accuracy:  0.5465\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 542, TN: 551, FP: 449, FN: 458\n",
            "TPR: 0.542, FPR: 0.449, one-point AUC:0.5465\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009071901440620422\n",
            "gamma: as per paper: 21.529389541807312\n",
            "Class: 4\n",
            "AUC score:  0.4777, Accuracy:  0.4825\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 491, TN: 474, FP: 526, FN: 509\n",
            "TPR: 0.491, FPR: 0.526, one-point AUC:0.4825\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009072614484466612\n",
            "gamma: as per paper: 21.527697482836736\n",
            "Class: 5\n",
            "AUC score:  0.5828, Accuracy:  0.5550\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 501, TN: 609, FP: 391, FN: 499\n",
            "TPR: 0.501, FPR: 0.391, one-point AUC:0.5549999999999999\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009035738767124712\n",
            "gamma: as per paper: 21.615554082928732\n",
            "Class: 6\n",
            "AUC score:  0.5861, Accuracy:  0.5620\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 503, TN: 621, FP: 379, FN: 497\n",
            "TPR: 0.503, FPR: 0.379, one-point AUC:0.562\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009062718600034714\n",
            "gamma: as per paper: 21.551204293075134\n",
            "Class: 7\n",
            "AUC score:  0.6303, Accuracy:  0.5960\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 536, TN: 656, FP: 344, FN: 464\n",
            "TPR: 0.536, FPR: 0.344, one-point AUC:0.5960000000000001\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009134216234087944\n",
            "gamma: as per paper: 21.382513287906857\n",
            "Class: 8\n",
            "AUC score:  0.6185, Accuracy:  0.5710\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 492, TN: 650, FP: 350, FN: 508\n",
            "TPR: 0.492, FPR: 0.35, one-point AUC:0.571\n",
            "\n",
            "np.var(OC_X_train_normalised):0.0009060995071195066\n",
            "gamma: as per paper: 21.55530363556858\n",
            "Class: 9\n",
            "AUC score:  0.6787, Accuracy:  0.6115\n",
            "OC_X_train shape: (5000, 512), X_test_cls shape: (2000, 512).\n",
            "Results after model application to *test* set: n_inlier: 1000 ; n_outlier: 1000\n",
            "TP: 490, TN: 733, FP: 267, FN: 510\n",
            "TPR: 0.49, FPR: 0.267, one-point AUC:0.6114999999999999\n",
            "\n",
            "Unweighted mean AUC:  0.5766\n",
            "Unweighted one-point mean AUC:  0.5499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coZy2dTKsDkq"
      },
      "source": [
        "#Run a Set of Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHgkq8JsJRY"
      },
      "source": [
        "The minimal example was easily extended to reproduce all results in from one cell, saving outputs to Google Drive.\n",
        "One run reproduction run will take about 1.5hr to run on Colab Pro with a P100; making five runs takes 7hr.  Code depends on your Google Drive file paths being specified correctly.  CSV of each run's results is saved, along with a second CSV with a rolling summary of runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsvAIjHROEf5"
      },
      "source": [
        "# import time\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# reproductions = 5\n",
        "# datasets = [\"fMNIST\", \"CIFAR10\", \"CIFAR100\"]\n",
        "# batch_size = 16\n",
        "# samp_per_cls = 500\n",
        "# random_seed = False\n",
        "# mdltypes = [\"resnet18\",\"resnet50\"]\n",
        "# save_runsummary = True\n",
        "# save_runsummary_collection = True\n",
        "\n",
        "# runsummary_collection = pd.DataFrame([])\n",
        "# for reproduction in range(reproductions):\n",
        "#   print(f\"Beginning reproduction {reproduction} of {reproductions}.\")\n",
        "#   for dataset in datasets:\n",
        "#     train_loader, test_loader = K9_dataloader(dataset=dataset,\n",
        "#                                               batch_size=batch_size)   #LOAD the data\n",
        "#     for mdltype in mdltypes:\n",
        "#       pretrained = True if mdltype == \"resnet50\" else False   #resnet50 pretrained, resnet18 random weights/untrained\n",
        "#       train_ft, train_lb, test_ft, test_lb = K9_resnet1850_ftex(train_loader,test_loader,mdltype=mdltype,pretrained=pretrained) #EXTRACT the features\n",
        "#       X_train, y_train = K9_stratified_class_sample(train_ft,train_lb,samp_per_cls=samp_per_cls,random_seed=random_seed) #REDUCE the sample size by stratified sampling\n",
        "#       X_test, y_test = test_ft, test_lb #KEEP test set the same size\n",
        "\n",
        "#       for kernel_type in [\"linear\",\"rbf\"]:\n",
        "#         res = K9_OCSVM_v4(X_train, y_train, X_test, y_test, kernel_type)  #Apply OC-SVM\n",
        "        \n",
        "#         run_summary = K9_pd_runsummary()  #Summarise the run\n",
        "#         print(run_summary)\n",
        "#         if save_runsummary: run_summary.to_csv(\"/content/gdrive/MyDrive/COMP6248CW/run_summary_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\")\n",
        "\n",
        "#         runsummary_collection = pd.concat([runsummary_collection,run_summary], axis = 1) #Append as a new column in runsummary\n",
        "#         if save_runsummary_collection: runsummary_collection.to_csv(\"/content/gdrive/MyDrive/COMP6248CW/run_summary_collection\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".csv\")"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}